---
title: "FORRT"
subtitle: "Framework for Open and Reproducible Research Training"
output: 
  html_document:
    css: style.css
---

***

<br>

<center>

## **A Framework for Evaluating and Incentivising the Teaching of Reproducible and Open Science[^1]**

[Sam Parsons](http://samdparsons.blogspot.com/), [Flavio Azevedo](http://flavioazevedo.com), [Carl Michael Galang](https://galangca.github.io/)

</center>

<div id="section" class="section level2 tabset tabset-fade" style="padding:50px">

## {.tabset .tabset-fade}

### Introduction

<div class="b">

<br>

Advancing science requires open, transparent, reproducible and cumulative research practices. Perhaps the oldest support for this dates back to the 1600's, when the newly founded Royal Society - world's oldest scientific institution - chose its motto be "*take nobody's word for it*". These fundamental features of science (McNutt, 2014; Miguel et al. 2014) increase the robustness of findings allowing for validity interpretation of results. Unfortunately, however, these practices have been shown to be underutilized in practice (Ioannidis, Munafo, Fasar-Poli, Nosek, & David, 2014; O’Boyle, Banks, & Ginzalez-Mulé, 2014). Worse, questionable research practices are common (Fiedler & Schwartz, 2016; John Lowenstein, & Prelec, 2012;), likely due to a misalignment of incentives for robust, replicable, and open research in the current academic publishing system (Bakker, van Dijk, & Wicherts, 2012; Higginson & Munafo, 2016). As a result, the replicability of previous research is in doubt (Franco, Malhotra, & Simonvits, 2014; Ioannidis, 2005; Open Science Collaboration, 2015; Simmons, Nelson, & Simonsohn, 2011) and has been characterized as the "replication crisis". The terminology of "credibility revolution" (Vazire, 2018), however, is more positive, reflecting widespread calls for improved practices, including - but not limited to - higher standards of evidence, preregistration, direct replication, transparency, and open science (e.g. Nosek, Ebersole, DeHaven, & Mellor, 2018; Nosek & Lakens, 2014; Zwaan, Etz, Lucas, & Donnellan, 2017; Wagenmakers, Wetzels, Borsboom, van der Maas, & Kievit, 2012). *À propos*, Brian Nosek and colleagues have recently unveiled the last iteration of they consider to be the triad pillars to usher the necessary changes towards academic nirvana: [opening scientific communication](https://arxiv.org/abs/1205.1055), [restructuring incentives and practices](http://journals.sagepub.com/doi/abs/10.1177/1745691612459058), and [crowdsourcing science](https://psyarxiv.com/vg649). 

**The problem**

But despite recent attention to meta-science, these perspectives have paid little attention to one key aspect of the scientific machinery: future scholars. Many of today's graduates and undergraduates go through their education without learning about the replication crisis, pre-registrations, cumulative science and other fundamental concepts that define modern science. As these better practices are not imbued at the onset of their careers - and if students only know what they are taught - it may be naive to expect the next generation of researchers will understand, value, and maintain the better institutional constraints and open research procedures implemented at the dawning of the reproducibility crisis. For example, if future scholars are not taught about the perils of HARKing, why would they care about pre-registering studies? The fact that it is still common practice to teach subject matters - at both the undergraduate and graduate levels - without emphasizing that scientific claims should be taken in light of probabilistic uncertainty, research design, samples and measurements used undermines the beneficial change achieved in recent years. At the very least, by upholding the status quo in teaching practices scientists risk missing out on generational leaps of scientific progress - if not a recurrence of the closed and irreplicable practices that gave rise to the scientific crisis in the first place. 

**A proposed solution** 

To avoid this scenario we argue that teaching reproducible and open research practices needs to be supported and incentivized. We propose that the teaching of reproducible and open research practices is the clearest indicator of the degree to which institutions and/or departments embody principles of credible science (and are not merely paying lip service). Here we present a framework to evaluate the quality of education on better research practices, as well as a pathway towards continuous improvement: a Framework for Open and Reproducible Research Training (FORRT). In this way, FORRT hopes to become an online source of teaching resources for faculty and institutions wanting to adapt the education of Psychological Science topics/subjects to the norms that would have precluded the current reproducibility crisis. For more details, see the second tab above "*FORRT as an Assessment tool*" in this page.


Taken a step further, FORRT intends to start a conversation on - and inspire debates about - the ethics of teaching substantive topics/subjects without regard for the reproducible crisis in Science more generally, in social sciences in particular. We argue that it is an ethical duty to teach the substantive foundations of a given subject-matter while contraposing its findings & claims to the newer and open scientific practice. In other words, that merging the teaching of substantive topics with open and reproducible means to abide by principled teaching and emphasis on inquiry-based methods for science teaching and learning. Which is to say, we contend that omitting to educate students about the replication crisis results in a false sense of certainty, which can be thought as misleading, if not unethical. Incidentally, the Ethical Principles of Psychologists and Code of Conduct of the American Psychological Association (APA) stipulates in Section 7, in Accuracy in Teaching, that “*when engaged in teaching or training, psychologists present psychological information accurately*”. In light of reproducibility crisis, we believe a possible interpretation of this guideline is one which is supportive of teaching subject-matters while communicating the possible caveats and uncertainty associated with any scientific work. This is further supported by the APA's ethical prescriptions on continuous self-improvement and education as a mean to teach Psychological Science accurately. *"Psychologists undertake ongoing efforts to develop and maintain their competence”* (Section 2.03); and *"Psychologists take reasonable steps to ensure that course syllabi are accurate regarding the subject matter to be covered, bases for evaluating progress, and the nature of course experiences"* (Section 7.03).

Indeed, the ideas instilled on the APA’s Code of Conduct portray the scientific enterprise as an continuous learning - and teaching - process, whose application transparency in science communication not only contributes to a more realistic portrayal of the subject matter but also helps shatter the academic glass ceiling - which ultimately could promote the curiosity and interest of students about the academic profession. Importantly, FORRT does not intend to advocate for the overburdening of scholars' schedules. In fact, one of its core missions is to facilitate this process by designing a framework that allows incremental improvements with a minimal informational encumberment, as well as providing the necessary resources, information, and tools. For more details, see the third tab above "*FORRT as a resource tool*" in this page.

FORRT provides a basis for evaluating the course content covered in relation to open and reproducible research. Alongside this evaluation there is a collection of resources aiming to help develop open and reproducible courses and further integrate these improved practices.


**Existing Initiatives**

FORRT was inspired, in part, from existing realizations of the credibility revolution, such as the Transparency and Openness Promotion Guidelines (TOP; Nosek et al. 2015). The TOP guidelines were developed for journals to evaluate and promote eight TOP standards in research practice (including: preregistration, data transparency, design and analysis transparency, and replication). The continuing aim is to align these critical features of a mature scientific discipline with practices required by journals and funding bodies. TOP achieves this with incremental levels of implementation of each standard; Disclose, Require, and Verify. This enables journals to select which level of each standard is most appropriate to their needs before implementation. The bar for entry is relatively low, and the levels offer guidance on how the journal may become more transparent and open in the future. At the time of writing, over 5000 journals and organisations have become TOP guidelines signatories, of which over 850 have implemented TOP (see https://cos.io/our-services/top-guidelines/ for a list of current signatories). We hope to achieve a similar goal with FORRT in providing a tool to evaluate teaching open and reproducible science alongside resources to further develop these courses. While TOP targets journals and funders, other efforts, such as the San Francisco Declaration on Research Assessment (DORA; https://sfdora.org/) target institutions and individual researchers. DORA advocates that academic institutions support improvements in research practices in research assessment. DORA proposes that institutions consider the scientific quality of all research outputs (e.g. datasets and software, in addition to journal articles) over metric based assessments, such as the number of publications, impact factors, or H-indices. Among other initiatives, TOP and DORA are important steps to incentivize and support researchers to engage in open and reproducible research practices. These guidelines highlight that improvements in research practices can be evaluated and incentivized. The onus is shared between journals, funders, and academic institutions to promote improvements in research practices. It is then the responsibility of individual researchers to remain up-to-date with these advancements and to meet these improved standards of open, transparent, and reproducible research. We hope that FORRT will meet the pressing need to evaluate and incentivise the teaching of these practices that has not yet been the focal point of other initiatives.

**FORRT**

Institutions and departments can support reproducible and open science through hiring researchers active in the credibility movement (we are happy to suggest many such individuals for active roles). Guest speakers and seminars can be hosted to promote improved practices, such as registered reports and open data. The mission statements and public profiles of universities can be amended to support open, reproducible science. However, to actively promote these practices and to embody these values, open and reproducible science must be taught as a core component of undergraduate courses. While FORRT is in its infancy, we developed it as an attempt to offer an unprecedented opportunity for teachers to shape the minds and future of the next generation of academics, towards open and reproducible science.

</div>

### FORRT as an Assessment tool

<br>

To be eligible, institutions or departments will be required to hold a public declaration on their support and commitment to the teaching of reproducible and open research practices. For example, the University of Surrey has such a statement on their website https://www.surrey.ac.uk/research/excellence/open-research

Our framework currently includes six principles core to the teaching of reproducible and open science practices:

* Reproducibility knowledge
* Conceptual and statistical knowledge
* Reproducible analyses
* Preregistration
* Open data and materials
* Replication research

Each principle will be evaluated on two dimensions: **Breadth** and **Depth**.

*Breadth* describes how widely teaching is distributed and takes three levels:

* **Not yet enacted, minimal breadth, or no evidence**: Although the principle may be discussed, it does not form part of a course / module component.
* **Opportunities for some**: The principle is taught, for example, as part of an optional course or elective or workshop.
* **Course requirement for all**: The principle is taught to all students, for example as part of the core course, compulsory module, or practical session.

*Depth* describes the degree to which students interact with the core principles and takes four levels:

* **Not yet enacted, minimal depth, or no evidence**: the curricula does not formally include teaching on the principle in question. Although instructors might mention the principle, it is not covered in depth and/or does not feature as part of the syllabi. 
* **Knowledge**: Students are taught about the principle and are required to demonstrate sufficient understanding.
* **Practice**: In addition to acquiring a knowledge base in the principle, students also put this understanding into practice in practical exercises.
* **Application**: In addition to acquiring a knowledge base in the principle and practical skills; students are required to apply this understanding and skill set in a research project.

Departments will be scored for each principle across the *breadth* and *depth* dimensions. A composite score will then be created for that department.

FORRT enables a contrast between course content as it currently stands and the ideal of teaching open and reproducible research practices as part of core training. Benchmarking the *breadth* and *depth* of course content on these principles of open and reproducible research enables course leaders to identify content that is lacking or can be improved. Note that while the highest standards for teaching these principles would be a course required the application of reproducible and open research practices, we acknowledge that this is not always possible. However, since FORTT is a tool intended to evaluate the current teaching practices for a wild variety of departments, its score should be able to convey and reflect low to high adherence to the teaching principles proposed by FORRT.

In addition to evaluating the quality of teaching, and some minor incentives using a badge/ranking system; this framework will act as a centralized platform containing resources for instructors wishing to implement reproducible and open science principles into their teaching.

To join FORRT please see the tab “Implementation Plan” in this page. For details about which universities/departments have joined FORRT and the courses they teach, please check our page on [Assessment](https://forrt.netlify.com/as.assessment.tool.html).


### FORRT as a resource tool

<br>

When making prescriptive claims, it is also important to supply stakeholders & interested parties the necessary tools for implementation. In this section we conceptualize FORRT as a source for reproducible and open-science teaching materials so that FORRT can also act towards the implementation and improvement of this research training. This is of fundamental importance because teachers’ and researchers’ time constraints are significant and implementing the necessary changes under these conditions would not be a trivial matter - no matter how sympathetic they may be to cause. As to address this, FORRT provides examples and links to existing resources and modules for each principle in the following way: for each principle FORRT provides a [curated list of resources](https://forrt.netlify.com/as.resource.tool.html) that have been vetted as suitable and fitting towards FORRT prescriptions. The goal of this curated list is to allow incremental improvements with a minimal informational encumberment. In other words, FORRT’s curated list is designed such as small time commitments can yield meaningful improvements in reproducible and open-science teaching. In addition, FORRT also provides a wider array of crowd-sourced teaching resources which can address sub-field specificities and peculiarities. For this [we provide a google form](https://docs.google.com/forms/d/e/1FAIpQLSeKGYgS6erRr41RZGx4VRiVuYdimWLPJmhguI3FrAHvOsLonQ/viewform) that feeds [a live database of resources](https://docs.google.com/spreadsheets/d/16G02hzkWEWa_6qGNMmacCiB5cCPGhTk9A_bjGfla3RE/edit?usp=sharing) which is open to the community’s input. The form is straightforward and can be easily filled in with modifiers so that the entered resources are appropriately categorized for easier retrieval. In doing so, we hope to sponsor general and specific contributions in terms of field/sub-field and FORRT principles, provide an always updated database, and promote integrative and cumulative scientific principles.


### Implementation Plan

<br>

Our implementation plan draws from the model of the Transparency and Openness Promotion guidelines (TOP). We hope that implementing FORRT will provide a public declaration of engagement with open and reproducible research in teaching. Likewise, once adoption has reached a critical threshold, lacking adoption also provides a public signal of non-engagement with these practices in research training. We hope that being awarded FORRT 'badges' (alternatively, "medallions", although we have been advised to avoid pandering) will be one incentive for adoption. FORRT goes beyond mere statements of ‘support for open science’. These statements are positive, however, unless the practices are enacted across a range of domains (with our current focus being teaching) this can be merely paying lip service. FORRT goes beyond recognition from statements of support towards a model in which engagement is rewarded, even if nominally by badges.

The first step to be eligible for FORRT is a public statement in support of open and reproducible science. Institutions, departments, and individual courses may be eligible for FORRT. As with adopting TOP (e.g. https://www.elsevier.com/journals/evolution-and-human-behavior/1090-5138/transparency-and-openness) to be classified in FORRT this statement must be expanded to describe the breadth and depth of engagement with each principle. This statement will need to be verified in some form and this may require direct links to supporting evidence (e.g. course syllabi). The FORRT breadth and width levels will be confirmed and the course will be added to a registrar - an announcement can then be made about the implementation and FORRT classification awarded. This will allow courses to be compared across the principles implemented, and at what degree of breadth and width. All FORRT adopters will be able to revise their statements as course content changes, with the lofty ideal of moving towards required application of open and reproducible science practices. 

Our implementation plan is as follows:

* Continue to develop FORRT. This paper is intended as a starting point to elicit widespread feedback about the framework and its implementation.
* Check with the community whether the pilot for the evaluation and its preliminary rankings can be improved upon. 
* Invite course leaders to become the first adopters of FORRT.



### References

<br>

Allie, Buffler, Campbell, Lubben, Evangelinos, Psillos, & Valassiades. (2003). Teaching measurement in the introductory physics laboratory, The Physics Teacher, 41(7), 394-401.

Bakker, M., vanDijk, A., & Wicherts, J. M. (2012). The Rules of the Game Called Psychological Science. Perspectives on Psychological Science, 7(6), 543–554. http://doi.org/10.1177/1745691612459060

Fiedler, K., & Schwarz, N. (2016). Questionable Research Practices Revisited. Social Psychological and Personality Science, 7(1), 45–52. http://doi.org/10.1177/1948550615612150

Franco, A., Malhotra, N., & Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. Science, 345(6203), 1502–1505. https://doi.org/10.1126/science.1255484

Higginson, A. D., & Munafo, M. R. (2016). Current incentives for scientists lead to underpowered studies with erroneous conclusions. PLoS Biology, 14(11), [e2000995]. DOI: 10.1371/journal.pbio.2000995

Ioannidis, J. P. A. (2005) Why most published research findings are false. PLoS Medicine, 2(8), 0696–0701. http://doi.org/10.1371/journal.pmed.0020124

Ioannidis, J. P. A., Munafo, M. R., Fusar-Poli, P., Nosek, B. A., & David, S. P. (2014). Publication and other reporting biases in cognitive sciences: Detection, prevalence, and prevention. Trends in Cognitive Sciences, 18. DOI: 10.1016/j.tics.2014.02.010

John, L.K., Loewenstein, & Prelec. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological science, 23(5), 524-532. DOI: 0.1177/0956797611430953.

McNutt, M. (2014). Reproducibility. Science, 343, 229. DOI: 10.1126/science.1250475

Nosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. http://doi.org/10.1073/pnas.1708274114

Nosek, B. A., & Lakens, D. (2014). Registered reports: A method to increase the credibility of published results. Social Psychology, 45(3), 137–141. http://doi.org/10.1027/1864-9335/a000192

O’Boyle, E. H., Banks, C. B., & Gonzalez-Mulé, E. (2014). The chrysalis effect: How ugly initial results metamorphosize into beautiful articles. Journal of management, 43(2), 376-399. DOI: 10.1177/0149206314527133

Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716. http://doi.org/10.1126/science.aac4716

Miguel, E., Camerer, C., Casey, K., Cohen, J., Esterling, K. M., Gerber, A., Glennerster, R., Green, D. P., Humphreys, M., Imbens, G., Laitin, D., Madon, T., Nelson, L., Nosek, B. A., Petersen, M., Sedlmayr, R., Simmons, J. P., Simonsohn, U., & Van der Laan, M. (2014). Promoting transparency in social science research. Science, 343, 30-31

Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting any thing as significant. Psychological Science, 22(11), 1359–1366. http://doi.org/10.1177/0956797611417632

Wagenmakers, E. J., Wetzels, R., Borsboom, D., van der Maas, H. L. J., & Kievit, R. A. (2012). An Agenda for Purely Confirmatory Research. Perspectives on Psychological Science, 7(6), 632–638. http://doi.org/10.1177/1745691612463078

Zwaan, R. A., Etz, A., Lucas, R. E., Donnellan, M. B. (2017). Making Replication Mainstream. Behavioral and Brain Sciences, 1-50. doi:10.1017/S0140525X17001972

<div>


<br>


[^1]: **Authors note**. *This project was initiated at the 2018 meeting of the Society for the Improvement of Psychological Science in the “Teaching replicable and reproducible science” hackathon led by Kristen Lane and Heather Urry. The initial framework was developed in a subsequent working group consisting of: Sam Parsons, Flavio Azevedo, Carl Michael Galang, Kristin Lane, Lisa DeBruine, Benjamin Le, Donald Tellinghuisen, and Madeline Harms (we apologise if we have missed anybody - please let us know). The [current version of the paper](https://docs.google.com/document/d/1DYAeQ2-veg4AQqLHMCdOyLoAU2WH2I_oaJIKc9StXXE/edit?ts=5b62ed12#) introducing FORRT was drafted by Sam Parsons & Flavio Azevedo, with integral feedback and support from Carl Michael Galang. The FORRT website has been designed - and is maintained - by Flavio Azevedo. To further develop FORRT we are seeking additional contributors and will endeavor to acknowledge all that provided feedback on this project.*