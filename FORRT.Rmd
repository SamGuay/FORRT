---
title: "FORRT"
subtitle: "Framework for Open and Reproducible Research Training"
output: 
  html_document:
    css: style.css
---

***

<br>

<center>

## **A Framework for Open and Reproducible Research Training[^1]**

[Sam Parsons](http://samdparsons.blogspot.com/), [Flavio Azevedo](http://flavioazevedo.com), [Carl Michael Galang](https://galangca.github.io/)

</center>

<div id="section" class="section level2 tabset tabset-fade" style="padding:50px">

## {.tabset .tabset-fade}

### Introduction

<div class="b">

<br>

Advancing science requires open, transparent, reproducible and cumulative research practices. Perhaps the oldest support for this dates back to the 1600's, when the newly founded Royal Society - world's oldest scientific institution - chose its motto be "*take nobody's word for it*". These fundamental features of science (McNutt, 2014; Miguel et al. 2014) increase the robustness of findings allowing for validity interpretation of results. Unfortunately, however, these practices have been shown to be underutilized in practice (Ioannidis, Munafo, Fasar-Poli, Nosek, & David, 2014; O’Boyle, Banks, & Ginzalez-Mulé, 2014). Worse, questionable research practices are common (Fiedler & Schwartz, 2016; John Lowenstein, & Prelec, 2012;), likely due to a misalignment of incentives for robust, replicable, and open research in the current academic publishing system (Bakker, van Dijk, & Wicherts, 2012; Higginson & Munafo, 2016). As a result, the replicability of previous research is in doubt (Franco, Malhotra, & Simonvits, 2014; Ioannidis, 2005; Open Science Collaboration, 2015; Simmons, Nelson, & Simonsohn, 2011) and has been characterized as the "replication crisis". The terminology of "credibility revolution" (Vazire, 2018), however, is more positive, reflecting widespread calls for improved practices, including - but not limited to - higher standards of evidence, preregistration, direct replication, transparency, and open science (e.g. Nosek, Ebersole, DeHaven, & Mellor, 2018; Nosek & Lakens, 2014; Zwaan, Etz, Lucas, & Donnellan, 2017; Wagenmakers, Wetzels, Borsboom, van der Maas, & Kievit, 2012). *À propos*, Brian Nosek and colleagues have recently unveiled the last iteration of they consider to be the triad pillars to usher the necessary changes towards academic nirvana: [opening scientific communication](https://arxiv.org/abs/1205.1055), [restructuring incentives and practices](http://journals.sagepub.com/doi/abs/10.1177/1745691612459058), and [crowdsourcing science](https://psyarxiv.com/vg649). 

**The problem**

But despite recent attention to meta-science, these perspectives have paid little attention to one key aspect of the scientific machinery: future scholars. Many of today's graduates and undergraduates go through their education without learning about the replication crisis, pre-registrations, cumulative science and other fundamental concepts that define modern science. As these better practices are not imbued at the onset of their careers - and if students only know what they are taught - it may be naive to expect the next generation of researchers will understand, value, and maintain the better institutional constraints and open research procedures implemented at the dawning of the reproducibility crisis. For example, if future scholars are not taught about the perils of HARKing, why would they care about pre-registering studies? The fact that it is still common practice to teach subject matters - at both the undergraduate and graduate levels - without emphasizing that scientific claims should be taken in light of probabilistic uncertainty, research design, samples and measurements used undermines the beneficial change achieved in recent years. At the very least, by upholding the status quo in teaching practices scientists risk missing out on generational leaps of scientific progress - if not a recurrence of the closed and irreplicable practices that gave rise to the scientific crisis in the first place. 

**A proposed solution** 

To avoid this scenario we argue that teaching reproducible and open research practices needs to be supported and incentivized. We propose that the teaching of reproducible and open research practices is the clearest indicator of the degree to which institutions and/or departments embody principles of credible science (and are not merely paying lip service). Here we present a framework to evaluate the quality of education on better research practices, as well as a pathway towards continuous improvement: a Framework for Open and Reproducible Research Training (FORRT). In this way, FORRT hopes to become an online source of teaching resources for faculty and institutions wanting to adapt the education of Psychological Science topics/subjects to the norms that would have precluded the current reproducibility crisis. For more details, see the second tab above "*FORRT as an Assessment tool*" in this page.


Taken a step further, FORRT intends to start a conversation on - and inspire debates about - the ethics of teaching substantive topics/subjects without regard for the reproducible crisis in Science more generally, in social sciences in particular. We argue that it is an ethical duty to teach the substantive foundations of a given subject-matter while contraposing its findings & claims to the newer and open scientific practice. In other words, that merging the teaching of substantive topics with open and reproducible means to abide by principled teaching and emphasis on inquiry-based methods for science teaching and learning. Which is to say, we contend that omitting to educate students about the replication crisis results in a false sense of certainty, which can be thought as misleading, if not unethical. Incidentally, the Ethical Principles of Psychologists and Code of Conduct of the American Psychological Association (APA) stipulates in Section 7, in Accuracy in Teaching, that “*when engaged in teaching or training, psychologists present psychological information accurately*”. In light of reproducibility crisis, we believe a possible interpretation of this guideline is one which is supportive of teaching subject-matters while communicating the possible caveats and uncertainty associated with any scientific work. This is further supported by the APA's ethical prescriptions on continuous self-improvement and education as a mean to teach Psychological Science accurately. *"Psychologists undertake ongoing efforts to develop and maintain their competence”* (Section 2.03); and *"Psychologists take reasonable steps to ensure that course syllabi are accurate regarding the subject matter to be covered, bases for evaluating progress, and the nature of course experiences"* (Section 7.03).

Indeed, the ideas instilled on the APA’s Code of Conduct portray the scientific enterprise as an continuous learning - and teaching - process, whose application transparency in science communication not only contributes to a more realistic portrayal of the subject matter but also helps shatter the academic glass ceiling - which ultimately could promote the curiosity and interest of students about the academic profession. Importantly, FORRT does not intend to advocate for the overburdening of scholars' schedules. In fact, one of its core missions is to facilitate this process by designing a framework that allows incremental improvements with a minimal informational encumberment, as well as providing the necessary resources, information, and tools. For more details, see the third tab above "*FORRT as a resource tool*" in this page.

FORRT provides a basis for evaluating the course content covered in relation to open and reproducible research. Alongside this evaluation there is a collection of resources aiming to help develop open and reproducible courses and further integrate these improved practices.


**Existing Initiatives**

FORRT was inspired, in part, from existing realizations of the credibility revolution, such as the Transparency and Openness Promotion Guidelines (TOP; Nosek et al. 2015). The TOP guidelines were developed for journals to evaluate and promote eight TOP standards in research practice (including: preregistration, data transparency, design and analysis transparency, and replication). The continuing aim is to align these critical features of a mature scientific discipline with practices required by journals and funding bodies. TOP achieves this with incremental levels of implementation of each standard; Disclose, Require, and Verify. This enables journals to select which level of each standard is most appropriate to their needs before implementation. The bar for entry is relatively low, and the levels offer guidance on how the journal may become more transparent and open in the future. At the time of writing, over 5000 journals and organisations have become TOP guidelines signatories, of which over 850 have implemented TOP (see https://cos.io/our-services/top-guidelines/ for a list of current signatories). We hope to achieve a similar goal with FORRT in providing a tool to evaluate teaching open and reproducible science alongside resources to further develop these courses. While TOP targets journals and funders, other efforts, such as the San Francisco Declaration on Research Assessment (DORA; https://sfdora.org/) target institutions and individual researchers. DORA advocates that academic institutions support improvements in research practices in research assessment. DORA proposes that institutions consider the scientific quality of all research outputs (e.g. datasets and software, in addition to journal articles) over metric based assessments, such as the number of publications, impact factors, or H-indices. Among other initiatives, TOP and DORA are important steps to incentivize and support researchers to engage in open and reproducible research practices. These guidelines highlight that improvements in research practices can be evaluated and incentivized. The onus is shared between journals, funders, and academic institutions to promote improvements in research practices. It is then the responsibility of individual researchers to remain up-to-date with these advancements and to meet these improved standards of open, transparent, and reproducible research. We hope that FORRT will meet the pressing need to evaluate and incentivise the teaching of these practices that has not yet been the focal point of other initiatives.

**FORRT**

Institutions and departments can support reproducible and open science through hiring researchers active in the credibility movement (we are happy to suggest many such individuals for active roles). Guest speakers and seminars can be hosted to promote improved practices, such as registered reports and open data. The mission statements and public profiles of universities can be amended to support open, reproducible science. However, to actively promote these practices and to embody these values, open and reproducible science must be taught as a core component of undergraduate courses. While FORRT is in its infancy, we developed it as an attempt to offer an unprecedented opportunity for teachers to shape the minds and future of the next generation of academics, towards open and reproducible science.

</div>


### Principles

<br>

FORRT is centred around a framework comprising six clusters of open and reproducible research practices. Each cluster has six sub-clusters.


#### **Cluster 1: Reproducibility crisis and Credibility Revolution**

**Summary**: Attainment of a grounding in the motivations and theoretical underpinnings of reproducible and open research. Integration with field specific content (i.e., or grounded in the history of replicability); 

* Replication crisis and credibility revolution
* Exploratory and confirmatory analyses
* Questionable research practices (their 'theory') and  prevalence
* Proposed improvement science initiatives on statistics, measurement, teaching, data sharing, code sharing, pre-registration, replication.
* Ongoing debates, (e.g. incentives for and against open science).
* Ethical considerations for improved practices.

Although not exhaustive, these concepts provide a broad coverage of this cluster. 


<center>

```{R, eval = FALSE, include = FALSE}
# I have put these into chunks so that they are not currently included. 
![](Principle1.png) 
```

</center>


#### **Cluster 2: Conceptual and Statistical Knowledge**

**Summary**: Enacting this principle indicates that students attain a grounding in fundamental statistics, measurement, and its implications. 

* The logic of null hypothesis testing, p-values, Type I and II errors (and when and why they might happen).
* Limitations and benefits of NHST, Bayesian and Likelihood approaches.
* Effect sizes, Statistical power, Confidence Intervals.
* Research Design, Sample Methods, and its implications for inferences.
* Questionable research (QRPs) & measurement practices (QMPs).
* Understand the relationship between all of the above.

Although not exhaustive, these concepts provide a broad coverage of this cluster.

<center>

```{r, eval = FALSE, include = FALSE}
![](Principle2.png) 
```

</center>




#### **Cluster 3: Reproducible analyses**



**Summary**: Reproducible analyses allow the checking of analytic pipelines and facilitate error correction. Enacting this principle requires students to move towards transparent and scripted analysis practices.

* Strengths of reproducible pipelines
* Scripted analyses compared with GUI
* Data wrangling
* Programming reproducible data analyses
* Open source and free softwares
* Tools to check yourself and others; statcheck, GRIM, and SPRITE


Although not exhaustive, these concepts provide a broad coverage of this cluster.

<center>

```{r, eval = FALSE, include = FALSE}
![](Principle3.png) 
```

</center>


#### **Cluster 4: Open data and materials**







**Summary**: Enacting this principle indicates that students have attained a grounding in open data and materials in both; using and sharing.

* Knowledge of traditional publication models. Open access publishing, preprints
* Reasons to share; for science, and for one’s own practices
* Repositories; e.g. OSF, FigShare, GitHub
* Accessing/sharing others data, code, and materials
* Ethical considerations
* Examples and consequences of accessing un/open data


Although not exhaustive, these concepts provide a broad coverage of this cluster.

<center>

```{r, eval = FALSE, include = FALSE}
![](Principle4.png)
```

</center>




#### **Cluster 5: Preregistration**



**Summary**: Preregistration entails laying out a complete methodology and analysis before a study has been undertaken. This facilitates transparency and removes several potential QRPs.



* Purpose of preregistration - distinguishing exploratory and confirmatory analyses, transparency measures
* Preregistration and registered reports - strengths and differences
* When can you preregister? Can you preregister secondary data?
* Writing a preregistration
* Comparing a preregistration to a final study manuscript
* Conducting a preregistered study



Although not exhaustive, these concepts provide a broad coverage of this cluster.

<center>

```{r, eval = FALSE, include = FALSE}
![](Principle5.png) 
```

</center>


#### **Cluster 6: Replication research**




**Summary**:  Replication research takes a variety of forms, each with a different purpose and contribution. Reproducible science requires replication research. 

* Purposes of replication attempts - what is a 'failed' replication?
* Large scale replication attempts
* Distinguishing conceptual and direct replications
* Conducting replication studies; challenges, limitations, and comparisons with the original study
* Registered Replication Reports
* The politics of replicating famous studies


Although not exhaustive, these concepts provide a broad coverage of this cluster.

<center>
```{r, eval = FALSE, include = FALSE}
![](Principle6.png) 
```

</center>

<div>



### Implementation Plan

<br>

Our implementation plan draws from the model of the Transparency and Openness Promotion guidelines (TOP). We hope that implementing FORRT will provide a public declaration of engagement with open and reproducible research in teaching. Likewise, once adoption has reached a critical threshold, lacking adoption also provides a public signal of non-engagement with these practices in research training. We hope that being awarded FORRT 'badges' (alternatively, "medallions", although we have been advised to avoid pandering) will be one incentive for adoption. FORRT goes beyond mere statements of ‘support for open science’. These statements are positive, however, unless the practices are enacted across a range of domains (with our current focus being teaching) this can be merely paying lip service. FORRT goes beyond recognition from statements of support towards a model in which engagement is rewarded, even if nominally by badges.

The first step to be eligible for FORRT is a public statement in support of open and reproducible science. Institutions, departments, and individual courses may be eligible for FORRT. As with adopting TOP (e.g. https://www.elsevier.com/journals/evolution-and-human-behavior/1090-5138/transparency-and-openness) to be classified in FORRT this statement must be expanded to describe the breadth and depth of engagement with each principle. This statement will need to be verified in some form and this may require direct links to supporting evidence (e.g. course syllabi). The FORRT breadth and width levels will be confirmed and the course will be added to a registrar - an announcement can then be made about the implementation and FORRT classification awarded. This will allow courses to be compared across the principles implemented, and at what degree of breadth and width. All FORRT adopters will be able to revise their statements as course content changes, with the lofty ideal of moving towards required application of open and reproducible science practices. 

Our implementation plan is as follows:

* Continue to develop FORRT. This paper is intended as a starting point to elicit widespread feedback about the framework and its implementation.
* Check with the community whether the pilot for the evaluation and its preliminary rankings can be improved upon. 
* Invite course leaders to become the first adopters of FORRT.



### References

<br>

Allie, Buffler, Campbell, Lubben, Evangelinos, Psillos, & Valassiades. (2003). Teaching measurement in the introductory physics laboratory, The Physics Teacher, 41(7), 394-401.

Bakker, M., vanDijk, A., & Wicherts, J. M. (2012). The Rules of the Game Called Psychological Science. Perspectives on Psychological Science, 7(6), 543–554. http://doi.org/10.1177/1745691612459060

Fiedler, K., & Schwarz, N. (2016). Questionable Research Practices Revisited. Social Psychological and Personality Science, 7(1), 45–52. http://doi.org/10.1177/1948550615612150

Franco, A., Malhotra, N., & Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. Science, 345(6203), 1502–1505. https://doi.org/10.1126/science.1255484

Higginson, A. D., & Munafo, M. R. (2016). Current incentives for scientists lead to underpowered studies with erroneous conclusions. PLoS Biology, 14(11), [e2000995]. DOI: 10.1371/journal.pbio.2000995

Ioannidis, J. P. A. (2005) Why most published research findings are false. PLoS Medicine, 2(8), 0696–0701. http://doi.org/10.1371/journal.pmed.0020124

Ioannidis, J. P. A., Munafo, M. R., Fusar-Poli, P., Nosek, B. A., & David, S. P. (2014). Publication and other reporting biases in cognitive sciences: Detection, prevalence, and prevention. Trends in Cognitive Sciences, 18. DOI: 10.1016/j.tics.2014.02.010

John, L.K., Loewenstein, & Prelec. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological science, 23(5), 524-532. DOI: 0.1177/0956797611430953.

McNutt, M. (2014). Reproducibility. Science, 343, 229. DOI: 10.1126/science.1250475

Nosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. http://doi.org/10.1073/pnas.1708274114

Nosek, B. A., & Lakens, D. (2014). Registered reports: A method to increase the credibility of published results. Social Psychology, 45(3), 137–141. http://doi.org/10.1027/1864-9335/a000192

O’Boyle, E. H., Banks, C. B., & Gonzalez-Mulé, E. (2014). The chrysalis effect: How ugly initial results metamorphosize into beautiful articles. Journal of management, 43(2), 376-399. DOI: 10.1177/0149206314527133

Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716. http://doi.org/10.1126/science.aac4716

Miguel, E., Camerer, C., Casey, K., Cohen, J., Esterling, K. M., Gerber, A., Glennerster, R., Green, D. P., Humphreys, M., Imbens, G., Laitin, D., Madon, T., Nelson, L., Nosek, B. A., Petersen, M., Sedlmayr, R., Simmons, J. P., Simonsohn, U., & Van der Laan, M. (2014). Promoting transparency in social science research. Science, 343, 30-31

Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting any thing as significant. Psychological Science, 22(11), 1359–1366. http://doi.org/10.1177/0956797611417632

Wagenmakers, E. J., Wetzels, R., Borsboom, D., van der Maas, H. L. J., & Kievit, R. A. (2012). An Agenda for Purely Confirmatory Research. Perspectives on Psychological Science, 7(6), 632–638. http://doi.org/10.1177/1745691612463078

Zwaan, R. A., Etz, A., Lucas, R. E., Donnellan, M. B. (2017). Making Replication Mainstream. Behavioral and Brain Sciences, 1-50. doi:10.1017/S0140525X17001972

<div>


<br>


[^1]: **Authors note**. *This project was initiated at the 2018 meeting of the Society for the Improvement of Psychological Science in the “Teaching replicable and reproducible science” hackathon led by Kristen Lane and Heather Urry. The initial framework was developed in a subsequent working group consisting of: Sam Parsons, Flavio Azevedo, Carl Michael Galang, Kristin Lane, Lisa DeBruine, Benjamin Le, Donald Tellinghuisen, and Madeline Harms (we apologise if we have missed anybody - please let us know). The [current version of the paper](https://docs.google.com/document/d/1DYAeQ2-veg4AQqLHMCdOyLoAU2WH2I_oaJIKc9StXXE/edit?ts=5b62ed12#) introducing FORRT was drafted by Sam Parsons & Flavio Azevedo, with integral feedback and support from Carl Michael Galang. The FORRT website has been designed - and is maintained - by Flavio Azevedo. To further develop FORRT we are seeking additional contributors and will endeavor to acknowledge all that provided feedback on this project.*