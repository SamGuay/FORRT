# Framework for Open and Reproducible Research Training (FORRT)

<br>
<center>

[Sam Parsons](http://samdparsons.blogspot.com/), [Flavio Azevedo](http://flavioazevedo.com), [Carl Michael Galang](https://galangca.github.io/)

<br>

Last updated: 14/09/2018. See https://forrt.netlify.com for the latest developments.

<br>

</center>


**FORRT** is an internation collaboration project, born out of SIPS 2018 (Society for the Improvement of Psychological Science), which aims to provide provide a framework for open and reproducible research training to scholars around the world. To learn more about FORRT, go to directly to the introduction (further below) and to learn more about how *you* can help or join us see our request for assistence immediately below. All the information below is on our website https://forrt.netlify.com, which should contain the latest developments and have a nicer feel to it


<br>

***

<br>

Dear collaborators, stakeholders, interested parties,

We present recommendations to improve the adoption of open science teaching practices by modifying the academic incentive structure (through ranking, badges) and by streamlining the education pipeline (through facilitating acquisition of resources). 

Despite questions about the implementation of open-science teaching guidelines, with FORRT psychologists now have an unprecedented opportunity to shape the minds and future of the next generation of academics.

We hope FORRT will become a useful tool in helping researchers improve the quality of education of open and reproducible Psychological Science. Our secondary goal is also to start a conversation on - and inspire debates about - the ethics of teaching substantive topics/subjects without regard for the reproducible crisis in Science more generally, in social sciences in particular.

Incidentally, Brian Nosek and colleagues have recently unveiled the last iteration of the three-part series on [Scientific Utopia](https://twitter.com/BrianNosek/status/1029105998911295488) in which they lay out the necessary changes towards academic nirvana: [opening scientific communication](https://arxiv.org/abs/1205.1055), [restructuring incentives and practices](http://journals.sagepub.com/doi/abs/10.1177/1745691612459058), and [crowdsourcing science](https://psyarxiv.com/vg649). We believe there should be a fourth pillar ensuring the new generation of scholars does not repeat the mistakes of its predecessors. And the way towards that objective is to familiarize students - and hence future academics - with the intricacies of science from the get-go. That is, introducing undergraduate and graduate students to foundational subject-matters *in tandem* with open & reproducible scientific principles can create an atmosphere of constructive curiosity and sustainable development of open and reproducible practices. And while FORRT is in its infancy, we developed it as an attempt to offer an unprecedented opportunity for teachers to shape the minds and future of the next generation of academics.

However, FORRT is still in its infancy. So as to maximize its potential scope and impact, we would like to request and encourage feedback at all stages of its development. In this sense, we would like your help in making FORRT a viable and implementable initiative. And for this, we would very much appreciate comments, edits, references, ideas. In specific, we have added throughout the website *"note from authors"*. And more generally, we have identified certain areas for which we would appreciate input:

* Are there other reasons why universities/departments should integrate the teaching of open and reproducible science - in addition to those laid out in the Introduction? And reasons they should not?
* Are there key concepts missing from the FORRT principles? Have we missed a concept important enough to become a principle of its own?
* Should FORRT be applied to individual courses, departments, institutions, or all three? And should recommendations change as part of this?
* Adoption for undergraduate, graduate, and doctoral programs - e.g. should we have a ‘sufficient’ level that differs for each training stage? Are there particular considerations we should give between courses?
* The way we are scoring universities/departments separates *breadth* & *depth* for each principle, we believe this separation is key as it conveys different kinds of information. Do you share the motivation behind this separation? And would a simpler version of scoring (e.g., aggregating across them) be better?


We consider this first attempt to be a preliminary, active, living idea and plan to update it contingent upon your feedback. You can provide feedback by reading the website (in order, the pages) [commenting our google document](https://docs.google.com/document/d/1DYAeQ2-veg4AQqLHMCdOyLoAU2WH2I_oaJIKc9StXXE/edit?ts=5b62ed12#), by sending an email to FORRTproject@gmail.com; or by emailing us directly: sam.parsons@psy.ox.ac.uk, azevedo@nyu.edu, galangc@mcmaster.ca.

Thank you for helping this initiative,

Sam, Flavio, Carl


<br>

***

<br>


### **FORRT: A Framework for Evaluating and Incentivising the Teaching of Reproducible and Open Science**

<br>

### Introduction

Advancing science requires open, transparent, cumulative and reproducible research practices. These fundamental features of science (McNutt, 2014; Miguel et al. 2014) increase the robustness of findings allowing for validity interpretation of results. Unfortunately, these practices have been shown to be underutilised in practice (Ioannidis, Munafo, Fasar-Poli, Nosek, & David, 2014; O’Boyle, Banks, & Ginzalez-Mulé, 2014). Worse, questionable research practices are common (Fiedler & Schwartz, 2016; John Lowenstein, & Prelec, 2012;), likely due to a misalignment of incentives for robust, replicable, and open research in the current academic publishing system (Bakker, van Dijk, & Wicherts, 2012; Higginson & Munafo, 2016). As a result, the replicability of previous research is in doubt (Franco, Malhotra, & Simonvits, 2014; Ioannidis, 2005; Open Science Collaboration, 2015; Simmons, Nelson, & Simonsohn, 2011) and has been characterised as the “replication crisis”. The terminology of “credibility revolution” (Vazire, 2018), however, is more positive, reflecting widespread calls for improved practices, including - but not limited to - higher standards of evidence, preregistration, direct replication, transparency, and open science (e.g. Nosek, Ebersole, DeHaven, & Mellor, 2018; Nosek & Lakens, 2014; Zwaan, Etz, Lucas, & Donnellan, 2017; Wagenmakers, Wetzels, Borsboom, van der Maas, & Kievit, 2012). 

One powerful realization of this scientific movement towards better science practices is the Transparency and Openness Promotion Guidelines (TOP; Nosek et al. 2015). The TOP guidelines were developed to evaluate and promote eight TOP standards in research practice (including: preregistration, data transparency, design and analysis transparency, and replication). The continuing aim is to align these critical features of a mature scientific discipline with practices required by journals and funding bodies. TOP achieves this with incremental levels of implementation of each standard; Disclose, Require, and Verify. This enables journals to select which level of each standard is most appropriate to their needs before implementation. The bar for entry is relatively low, and the levels offer guidance on how the journal may become more transparent and open in the future. At the time of writing, over 5000 journals and organisations have become TOP guidelines signatories, of which over 850 have implemented TOP (see https://cos.io/our-services/top-guidelines/ for a list of current signatories). 
Other efforts, such as the San Francisco Declaration on Research Assessment (DORA; https://sfdora.org/), have advocated that academic institutions support improvements in research practices in research assessment. DORA proposes that institutions consider the scientific quality of all research outputs (e.g. datasets and software, in addition to journal articles) over metric based assessments, such as the number of publications, Impact factors, or H-indices. TOP and DORA, amongst others, are important steps to incentivise and support researchers to engage in open and reproducible research practices. These guidelines highlight that improvements in research practices can be evaluated and incentivised. The onus is shared between journals, funders, and academic institutions to promote improvements in research practices. It is then the responsibility of individual researchers to remain up-to-date with these advancements and to meet these improved standards of open, transparent, and reproducible research. 

One could argue that it is reasonable to expect active researchers to have an understanding of the credibility revolution as they are self-interested stakeholders. However, undergraduate students (and to a large extent graduate students) go through their education process without hearing about the 'replication crisis' or 'pre-registration', and as these students can only be expected to know what they are taught, we cannot hold them responsible for their lack of knowledge on open and reproducible science.

Undergraduate and graduate students are the next generation of researchers.They will inherit the institutions and research procedures we set forth, and should ultimately become the standard bearer of open science and reproducibility. For this reason,  the lessons learned by their predecessors must be taught as a central components of the research process. Especially, while teaching the substantive foundations of a given field/subject by contraposing its findings & claims to the newer and open scientific practice. This is relevant because students don't always understand that scientific claims should be taken in light of probabilistic uncertainty, research design, and samples and measurements used (Allie et al, 2003). A step further, educating students with a false sense of certainty is misleading and unethical. Indeed, it is stipulated in the Ethical Principles of Psychologists and Code of Conduct of the American Psychological Association (APA), Section 7, Accuracy in Teaching, that “when engaged in teaching or training, psychologists present psychological information accurately”. In light of reproducibility crisis, this means teaching the subject matter while communicating the possible caveats and uncertainty associated with any scientific work. Transparency in teaching not only contributes to a more realistic portrayal of the scientific enterprise, but also helps shatter the academic glass ceiling - which ultimately could promote the curiosity and interest of students about the academic profession. While FORRT does not intend to advocate for the overburdening of academics, the necessity of self-improvement and continued education as a mean to teach Psychological Science accurately is instilled in at least two separate occasions on the APA’s Code of Conduct: “Psychologists undertake ongoing efforts to develop and maintain their competence” (Section 2.03); “Psychologists take reasonable steps to ensure that course syllabi are accurate regarding the subject matter to be covered, bases for evaluating progress, and the nature of course experiences” (Section 7.03).  

The reality of the matter, however, is that these practices are not imbued at the onset of new researchers’ careers. In doing so, we risk a recurrence of the closed and irreplicable practices that gave rise to the credibility revolution. To avoid this scenario - not to mention abiding by principled teaching and emphasis on inquiry-based methods for science teaching and learning - we argue that teaching reproducible and open research practices needs to be supported and incentivised. We propose that the teaching of reproducible and open research practices is the clearest indicator of the degree to which institutions and/or departments embody principles of credible science. And here we present a framework to evaluate the quality of education on better research practices, as well as a pathway towards ongoing improvement - the Framework for Open and Reproducible Research Training (FORRT). FORRT hopes to become an online source of teaching resources for faculty and institutions wanting to adapt the education of Psychological Science topics/subjects to the norms that would have precluded the current reproducibility crisis. 

<br><br>

### FORRT as an Assessment tool


To be eligible, institutions or departments will be required to hold a public declaration on their support and commitment to the teaching of reproducible and open research practices. For example, the University of Surrey has such a statement on their website https://www.surrey.ac.uk/research/excellence/open-research

Our framework currently includes six principles core to the teaching of reproducible and open science practices:

* Reproducibility knowledge
* Conceptual and statistical knowledge
* Reproducible analyses
* Preregistration
* Open data and materials
* Replication research

Each principle will be evaluated on two dimensions: **Breadth** and **Depth**.

*Breadth* describes how widely teaching is distributed and takes three levels:

* **Not yet enacted, minimal breadth, or no evidence**: Although the principle may be discussed, it does not form part of a course / module component.
* **Opportunities for some**: The principle is taught, for example, as part of an optional course or elective or workshop.
* **Course requirement for all**: The principle is taught to all students, for example as part of the core course, compulsory module, or practical session.

*Depth* describes the degree to which students interact with the core principles and takes four levels:

* **Not yet enacted, minimal depth, or no evidence**: the curricula does not formally include teaching on the principle in question. Although instructors might mention the principle, it is not covered in depth and/or does not feature as part of the syllabi. 
* **Knowledge**: Students are taught about the principle and are required to demonstrate sufficient understanding.
* **Practice**: In addition to acquiring a knowledge base in the principle, students also put this understanding into practice in practical exercises.
* **Application**: In addition to acquiring a knowledge base in the principle and practical skills; students are required to apply this understanding and skill set in a research project.

Departments will be scored for each principle across the breadth and depth dimensions. A composite score will then be created for that department.
FORRT enables a contrast between course content as it currently stands and the ideal of teaching open and reproducible research practices as part of core training. Benchmarking the breadth and depth of course content on these principles of open and reproducible research enables course leaders to identify content that is lacking or can be improved. Note that while the highest standards for teaching these principles would be a course required the application of reproducible and open research practices, we acknowledge that this is not always possible. However, since FORTT is a tool intended to evaluate the current teaching practices for a wild variety of departments, its score should be able to convey and reflect low to high adherence to the teaching principles proposed by FORRT.

In addition to evaluating the quality of teaching, and some minor incentives using a badge/ranking system; this framework will act as a resource for instructors wishing to implement reproducible and open science principles into their teaching.
 

<br><br>

### FORRT as an resource tool


When making prescriptive claims, it is also important to supply stakeholders & interested parties the necessary tools for implementation. In this section we conceptualize FORRT as a source for reproducible and open-science teaching materials so that FORRT can also act towards the implementation and improvement of this research training. This is of fundamental importance because teachers’ and researchers’ time constraints are significant and implementing the necessary changes under these conditions would not be a trivial matter - no matter how sympathetic they may be to cause. As to address this, FORRT provides examples and links to existing resources and modules for each principle in the following way: for each principle FORRT provides a [curated list of X resources]() that have been vetted as suitable and fitting towards FORRT prescriptions. The goal of this curated list is to allow incremental improvements with a minimal informational encumberment. In other words, FORRT’s curated list is designed such as small time commitments can yield meaningful improvements in reproducible and open-science teaching. In addition, FORRT also provides a wider array of crowdsourced teaching resources which can address sub-field specificities and peculiarities. For this [we provide a google form](https://docs.google.com/forms/d/e/1FAIpQLSeKGYgS6erRr41RZGx4VRiVuYdimWLPJmhguI3FrAHvOsLonQ/viewform) that feeds [a live database of resources](https://docs.google.com/spreadsheets/d/16G02hzkWEWa_6qGNMmacCiB5cCPGhTk9A_bjGfla3RE/edit?usp=sharing) which is open to the community’s input. The form is straightforward and can be easily filled in with modifiers so that the entered resources are appropriately categorized for easier retrieval. In doing so, we hope to sponsor general and specific contributions in terms of field/sub-field and FORRT principles, provide an always updated database, and promote integrative and cumulative scientific principles.

<br><br>

### Implementation Plan

*Note: this implementation plan is somewhat ‘idealised’, and we would appreciate any feedback (also see request for assistance cover note).*

Our implementation plan draws from the model of the Transparency and Openness Promotion guidelines (TOP). We hope that implementing FORRT will provide a public declaration of engagement with open and reproducible research in teaching. Likewise, once adoption has reached a critical threshold, lacking adoption also provides a public signal of non-engagement with these practices in research training. We hope that being awarded FORRT ‘badges’ (alternatively, “medallions”, although we have been advised to avoid pandering) will be one incentive for adoption. FORRT goes beyond mere statements of ‘support for open science’. These statements are positive, however, unless the practices are enacted across a range of domains (with our current focus being teaching) this can be merely paying lip service. FORRT goes beyond recognition from statements of support towards a model in which engagement is rewarded, even if nominally by badges.

The first step to be eligible for FORRT is a public statement in support of open and reproducible science. Institutions, departments, and individual courses may be eligible for FORRT. As with adopting TOP (e.g. https://www.elsevier.com/journals/evolution-and-human-behavior/1090-5138/transparency-and-openness) to be classified in FORRT this statement must be expanded to describe the breadth and depth of engagement with each principle. This statement will need to be verified in some form and this may require direct links to supporting evidence (e.g. course syllabi). The FORRT breadth and width levels will be confirmed and the course will be added to a registrar - an announcement can then be made about the implementation and FORRT classification awarded. This will allow courses to be compared across the principles implemented, and at what degree of breadth and width. All FORRT adopters will be able to revise their statements as course content changes, with the lofty ideal of moving towards required application of open and reproducible science practices. 

At the time of submitting this version our implementation plan is as follows:

* Continue to develop FORRT. This paper is intended as a starting point to elicit widespread feedback about the framework and its implementation.
* Develop detailed example of an evidence document for the revised framework
* Pilot the evaluation and create preliminary rankings. This will then provide a template for completed FORRT statements in the future. 
* Invite course leaders to become the first adopters of FORRT

<br><br>

### References

Allie, Buffler, Campbell, Lubben, Evangelinos, Psillos, & Valassiades. (2003). Teaching measurement in the introductory physics laboratory, The Physics Teacher, 41(7), 394-401.

Bakker, M., vanDijk, A., & Wicherts, J. M. (2012). The Rules of the Game Called Psychological Science. Perspectives on Psychological Science, 7(6), 543–554. http://doi.org/10.1177/1745691612459060

Fiedler, K., & Schwarz, N. (2016). Questionable Research Practices Revisited. Social Psychological and Personality Science, 7(1), 45–52. http://doi.org/10.1177/1948550615612150

Franco, A., Malhotra, N., & Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. Science, 345(6203), 1502–1505. https://doi.org/10.1126/science.1255484

Higginson, A. D., & Munafo, M. R. (2016). Current incentives for scientists lead to underpowered studies with erroneous conclusions. PLoS Biology, 14(11), [e2000995]. DOI: 10.1371/journal.pbio.2000995

Ioannidis, J. P. A. (2005) Why most published research findings are false. PLoS Medicine, 2(8), 0696–0701. http://doi.org/10.1371/journal.pmed.0020124

Ioannidis, J. P. A., Munafo, M. R., Fusar-Poli, P., Nosek, B. A., & David, S. P. (2014). Publication and other reporting biases in cognitive sciences: Detection, prevalence, and prevention. Trends in Cognitive Sciences, 18. DOI: 10.1016/j.tics.2014.02.010

John, L.K., Loewenstein, & Prelec. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological science, 23(5), 524-532. DOI: 0.1177/0956797611430953.

McNutt, M. (2014). Reproducibility. Science, 343, 229. DOI: 10.1126/science.1250475

Nosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. http://doi.org/10.1073/pnas.1708274114

Nosek, B. A., & Lakens, D. (2014). Registered reports: A method to increase the credibility of published results. Social Psychology, 45(3), 137–141. http://doi.org/10.1027/1864-9335/a000192

O’Boyle, E. H., Banks, C. B., & Gonzalez-Mulé, E. (2014). The chrysalis effect: How ugly initial results metamorphosize into beautiful articles. Journal of management, 43(2), 376-399. DOI: 10.1177/0149206314527133

Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716. http://doi.org/10.1126/science.aac4716

Miguel, E., Camerer, C., Casey, K., Cohen, J., Esterling, K. M., Gerber, A., Glennerster, R., Green, D. P., Humphreys, M., Imbens, G., Laitin, D., Madon, T., Nelson, L., Nosek, B. A., Petersen, M., Sedlmayr, R., Simmons, J. P., Simonsohn, U., & Van der Laan, M. (2014). Promoting transparency in social science research. Science, 343, 30-31

Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting any thing as significant. Psychological Science, 22(11), 1359–1366. http://doi.org/10.1177/0956797611417632

Wagenmakers, E. J., Wetzels, R., Borsboom, D., van der Maas, H. L. J., & Kievit, R. A. (2012). An Agenda for Purely Confirmatory Research. Perspectives on Psychological Science, 7(6), 632–638. http://doi.org/10.1177/1745691612463078

Zwaan, R. A., Etz, A., Lucas, R. E., Donnellan, M. B. (2017). Making Replication Mainstream. Behavioral and Brain Sciences, 1-50. doi:10.1017/S0140525X17001972